task: train_augment
slurm:
  time: 4320
  partition: dcs-gpu
  account: dcs-res
  mem_per_gpu: 64
  gpus: 1
  setup:
  - module load SoX/14.4.2-GCC-8.3.0
  - module load Anaconda3/5.3.0
  - source activate speech-diff
  srun_args:
  - --export=ALL
  cpus_per_task: 1
  cpus_per_gpu: 1
dora:
  dir: outputs
augment:
  fastspeech2:
    model:
      transformer:
        encoder_layer: 4
        encoder_head: 2
        encoder_hidden: 256
        decoder_layer: 6
        decoder_head: 2
        decoder_hidden: 256
        conv_filter_size: 1024
        conv_kernel_size:
        - 9
        - 1
        encoder_dropout: 0.2
        decoder_dropout: 0.2
      variance_predictor:
        filter_size: 256
        kernel_size: 3
        dropout: 0.5
      variance_embedding:
        pitch_quantization: linear
        energy_quantization: linear
        n_bins: 256
      multi_speaker: true
      max_seq_len: 1000
      vocoder:
        model: HiFi-GAN
        speaker: universal
        config_path: /fastdata/acq22mc/exp/dasr/dasr/tts/FastSpeech2/hifigan
    preprocess:
      dataset: null
      path:
        corpus_path: null
        lexicon_path: /fastdata/acq22mc/exp/dasr/dasr/tts/FastSpeech2/lexicon/librispeech-lexicon.txt
        raw_path: null
        preprocessed_path: null
      preprocessing:
        val_size: 512
        text:
          text_cleaners:
          - english_cleaners
          language: en
        audio:
          sampling_rate: 16000
          max_wav_value: 32768.0
        stft:
          filter_length: 1024
          hop_length: 256
          win_length: 1024
        mel:
          n_mel_channels: 80
          mel_fmin: 0
          mel_fmax: 8000
        pitch:
          feature: phoneme_level
          normalization: true
        energy:
          feature: phoneme_level
          normalization: true
    train:
      path:
        ckpt_path: null
        log_path: null
        result_path: null
      optimizer:
        batch_size: 16
        betas:
        - 0.9
        - 0.98
        eps: 1.0e-09
        weight_decay: 0.0
        grad_clip_thresh: 1.0
        grad_acc_step: 1
        warm_up_step: 4000
        anneal_steps:
        - 300000
        - 400000
        - 500000
        anneal_rate: 0.3
      step:
        total_step: 900000
        log_step: 100
        synth_step: 1000
        val_step: 1000
        save_step: 10000
  name: fastspeech2
data:
  name: torgo
  csv_root: /fastdata/acq22mc/exp/dasr/manifest/TORGO.csv
  root: /fastdata/acq22mc/data/dysarthria/Torgo_use
